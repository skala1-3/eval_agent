# agents/rag_retriever_agent.py

import os
import logging
from typing import List, Dict, Tuple
from openai import OpenAI
import chromadb

# rank-bm25ê°€ ì—†ë”ë¼ë„ ë™ì‘í•˜ë„ë¡ í´ë°± ì§€ì›
try:
    from rank_bm25 import BM25Okapi  # pip install rank-bm25

    _HAS_BM25 = True
except Exception:
    BM25Okapi = None
    _HAS_BM25 = False

from graph.state import PipelineState, CompanyMeta, Evidence, EvidenceCategory

logger = logging.getLogger(__name__)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¶•ë³„ í‚¤ì›Œë“œ(ê°€ë²¼ìš´ ì¶• ì í•©ì„± ë¶€ìŠ¤íŠ¸/í´ë°± ë­í‚¹ì— ì‚¬ìš©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
AXIS_KEYWORDS: Dict[EvidenceCategory, List[str]] = {
    "ai_tech": [
        "model",
        "llm",
        "inference",
        "rag",
        "fine-tune",
        "architecture",
        "on-prem",
        "latency",
        "embedding",
    ],
    "market": ["tam", "cagr", "market", "growth", "adoption", "segment", "share"],
    "traction": ["arr", "mrr", "revenue", "customers", "users", "contracts", "logo", "paying"],
    "moat": ["patent", "proprietary", "moat", "barrier", "exclusive", "differentiation"],
    "risk": ["regulation", "regulatory", "sec", "compliance", "privacy", "liability", "risk"],
    "team": ["founder", "ceo", "cto", "background", "experience", "hiring", "headcount"],
    "deployability": [
        "integration",
        "sla",
        "on-prem",
        "vpc",
        "security",
        "soc2",
        "iso27001",
        "sso",
        "audit",
    ],
}


def _norm(xs: List[float]) -> List[float]:
    if not xs:
        return []
    lo, hi = min(xs), max(xs)
    if hi == lo:
        return [0.0 for _ in xs]
    return [(x - lo) / (hi - lo) for x in xs]


def _distance_to_similarity(dists: List[float]) -> List[float]:
    # chroma distance â†’ ê°„ë‹¨ ìœ ì‚¬ë„(ê°€ê¹Œìš¸ìˆ˜ë¡ í¬ë‹¤)
    return [1.0 / (1.0 + float(d)) for d in dists]


def _keyword_hits(text: str, kws: List[str]) -> int:
    t = (text or "").lower()
    return sum(1 for k in kws if k and k.lower() in t)


def _bm25_scores(query: str, docs: List[str], axis: str) -> List[float]:
    """BM25 ì ìˆ˜(ì—†ìœ¼ë©´ ê²½ëŸ‰ í‚¤ì›Œë“œ ì¹´ìš´íŠ¸ë¡œ ëŒ€ì²´)."""
    q = (query + " " + " ".join(AXIS_KEYWORDS.get(axis, []))).lower()
    if _HAS_BM25:
        tokens = [(d or "").lower().split() for d in docs]
        bm25 = BM25Okapi(tokens)
        return list(bm25.get_scores(q.split()))
    # í´ë°±: ë‹¨ìˆœ í‚¤ì›Œë“œ íˆíŠ¸ ìˆ˜(ì •ê·œí™” ì „ ì›ì‹œê°’)
    kws = AXIS_KEYWORDS.get(axis, [])
    return [_keyword_hits(d or "", [*kws, *q.split()]) for d in docs]


def _hybrid_rank(
    search_query: str, docs: List[str], dists: List[float], axis: EvidenceCategory
) -> Tuple[List[int], List[float]]:
    sims = _distance_to_similarity(dists)  # ë²¡í„° ìœ ì‚¬ë„
    bm25 = _bm25_scores(search_query, docs, axis)  # ë ‰ì‹œì»¬ ì ìˆ˜
    kw = [
        _keyword_hits(d or "", AXIS_KEYWORDS.get(axis, [])) for d in docs
    ]  # ì•½í•œ ì¶• í‚¤ì›Œë“œ ë¶€ìŠ¤íŠ¸

    sim_n = _norm(sims)
    bm25_n = _norm(bm25)
    kw_n = _norm(kw)

    final = [0.6 * sn + 0.35 * bn + 0.05 * kn for sn, bn, kn in zip(sim_n, bm25_n, kw_n)]
    order = sorted(range(len(docs)), key=lambda i: final[i], reverse=True)
    return order, final


class RAGRetrieverAgent:
    """
    íšŒì‚¬/ì¶•ë³„ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(ë²¡í„° TopK â†’ BM25/í‚¤ì›Œë“œ ì¬ë­í¬ â†’ Evidence ìƒì„±)
    """

    def __init__(
        self, db_path: str | None = None, collection_name: str = "financial_companies_evidence"
    ):
        print("ğŸ¤– RAGRetrieverAgent ì´ˆê¸°í™” ì¤‘...")
        self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        db_path = db_path or os.path.join(os.getcwd(), "db", "chroma_db")
        self.db_client = chromadb.PersistentClient(path=db_path)
        self.collection = self.db_client.get_collection(name=collection_name)
        print(f"âœ… ChromaDB ì»¬ë ‰ì…˜ '{collection_name}'ì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°í–ˆìŠµë‹ˆë‹¤.")

    def __call__(self, state: PipelineState) -> PipelineState:
        return self.invoke(state)

    def _generate_search_query(self, axis: str, company_name: str) -> str:
        prompt = f"""
        ë‹¹ì‹ ì€ íˆ¬ì ì‹¬ì‚¬ì—­ì„ ìœ„í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ìŠ¤íƒ€íŠ¸ì—… '{company_name}'ì— ëŒ€í•´ ë‹¤ìŒ í‰ê°€ í•­ëª©ì„ ê²€ì¦í•˜ê¸° ìœ„í•œ ê°€ì¥ íš¨ê³¼ì ì¸ ê²€ìƒ‰ ì§ˆë¬¸ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
        ì§ˆë¬¸ì€ í•´ë‹¹ ìŠ¤íƒ€íŠ¸ì—…ì˜ êµ¬ì²´ì ì¸ ì •ë³´(ê¸°ìˆ , ì‹œì¥, íŒ€, ì¬ë¬´ ë“±)ë¥¼ ìµœëŒ€í•œ ì´ëŒì–´ë‚¼ ìˆ˜ ìˆë„ë¡ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
        í‰ê°€ í•­ëª©: "{axis}"
        """
        try:
            res = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=100,
            )
            return res.choices[0].message.content.strip().strip('"')
        except Exception as e:
            logger.warning(f"ì¿¼ë¦¬ ìƒì„± ì˜¤ë¥˜(axis={axis}): {e}")
            return f"{company_name} {axis}"

    def _embed_query(self, text: str) -> List[float]:
        return (
            self.openai_client.embeddings.create(input=[text], model="text-embedding-3-small")
            .data[0]
            .embedding
        )

    def invoke(self, state: PipelineState) -> PipelineState:
        print(f"\nğŸ” ì´ {len(state.companies)}ê°œ íšŒì‚¬ì— ëŒ€í•œ ê·¼ê±° ìë£Œ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        all_companies_evidence: Dict[str, Dict[str, List[Evidence]]] = {}

        evaluation_axes: Dict[EvidenceCategory, str] = {
            "ai_tech": "ê¸°ìˆ  í˜ì‹ ì„± ë° ë…ì°½ì„±",
            "market": "ì‹œì¥ ì ì¬ë ¥ ë° ì„±ì¥ì„±",
            "team": "íŒ€ ì—­ëŸ‰ ë° ì „ë¬¸ì„±",
            "moat": "ê²½ìŸ í™˜ê²½ ë° ì°¨ë³„ì„±",
            "risk": "ê·œì œ ë° ë²•ì  ë¦¬ìŠ¤í¬",
            "traction": "ì‚¬ì—… ì„±ê³¼ ë° ë§¤ì¶œ/ì§€í‘œ",
            "deployability": "ë„ì… ìš©ì´ì„±Â·ë³´ì•ˆÂ·ìš´ì˜",
        }

        TOPK = 50  # ë²¡í„° 1ì°¨ í›„ë³´
        TOPN = 3  # ìµœì¢… Evidence ê°œìˆ˜

        for company in state.companies:
            print(f"\n  ğŸ¢ '{company.name}' (ID: {company.id}) ì²˜ë¦¬ ì¤‘...")
            ev_per_axis: Dict[str, List[Evidence]] = {}

            for axis_key, axis_desc in evaluation_axes.items():
                # 1) ì¿¼ë¦¬ + ì„ë² ë”©
                q = self._generate_search_query(axis_desc, company.name)
                try:
                    qvec = self._embed_query(q)
                except Exception as e:
                    logger.warning(f"ì„ë² ë”© ì‹¤íŒ¨(axis={axis_key}, company={company.id}): {e}")
                    ev_per_axis[axis_key] = []
                    continue

                try:
                    # 2) ë²¡í„° TopK (+ íšŒì‚¬ í•„í„°)
                    res = self.collection.query(
                        query_embeddings=[qvec],
                        n_results=TOPK,
                        where={"company_id": company.id},
                        include=["documents", "metadatas", "distances"],
                    )
                    docs = res.get("documents", [[]])[0] or []
                    metas = res.get("metadatas", [[]])[0] or []
                    dists = res.get("distances", [[]])[0] or []

                    if not docs:
                        ev_per_axis[axis_key] = []
                        continue

                    # 3) í•˜ì´ë¸Œë¦¬ë“œ ì¬ë­í¬
                    order, final = _hybrid_rank(q, docs, dists, axis_key)

                    # 4) Evidence êµ¬ì„±(+ ì¶• í‚¤ì›Œë“œ ë¶€ìŠ¤íŠ¸ì— ë”°ë¥¸ weakâ†’medium ìŠ¹ê¸‰)
                    evs: List[Evidence] = []
                    axis_kws = AXIS_KEYWORDS.get(axis_key, [])
                    for i in order[:TOPN]:
                        text, meta = docs[i], metas[i]
                        strength = meta.get("strength", "weak")
                        if strength == "weak" and _keyword_hits(text, axis_kws) >= 2:
                            strength = "medium"
                        evs.append(
                            Evidence(
                                source=meta.get("source", "Unknown"),
                                text=text,
                                category=meta.get("category", axis_key),
                                strength=strength,
                                published=meta.get("published"),
                            )
                        )
                    ev_per_axis[axis_key] = evs

                except Exception as e:
                    print(f"      - ğŸš¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    ev_per_axis[axis_key] = []

            axis_counts = {k: len(v) for k, v in ev_per_axis.items()}
            logging.info(
                f"[RAG] retrieved[{company.id}] axis_counts={axis_counts} total={sum(axis_counts.values())}"
            )
            all_companies_evidence[company.id] = ev_per_axis

        state.retrieved_evidence = all_companies_evidence
        print("\nâœ… ëª¨ë“  íšŒì‚¬ì— ëŒ€í•œ ê·¼ê±° ìë£Œ ê²€ìƒ‰ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
        return state


# --- ë¡œì»¬ í…ŒìŠ¤íŠ¸ ---
if __name__ == "__main__":
    try:
        agent = RAGRetrieverAgent()
        init = PipelineState(
            query="AI financial advisory startup",
            companies=[
                CompanyMeta(id="acme-corp", name="Acme Corp", website="https://acme.example.com"),
                CompanyMeta(
                    id="beta-fi", name="Beta Finance", website="https://betafi.example.com"
                ),
            ],
        )
        out = agent(init)
        for company_id, evmap in out.retrieved_evidence.items():
            print(f"\nğŸ¢ Company ID: {company_id}")
            for axis, evs in evmap.items():
                print(f"  - {axis} ({len(evs)}ê°œ)")
                for ev in evs:
                    print(f"    â€¢ [{ev.strength}] {ev.source} :: {ev.text[:80]}...")
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
